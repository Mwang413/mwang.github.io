{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       "  'positive'],\n",
       " ['@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       "  'positive'],\n",
       " ['@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       "  'positive'],\n",
       " ['@97sides CONGRATS :)', 'positive'],\n",
       " ['yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days',\n",
       "  'positive']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Tweets: I included the original judgements (\"positive\" and \"negative\") given to the Tweets,\n",
    "#      in order to test them against the judgements of my model that I've built for its accuracy.\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "Tweets = ([[t, \"positive\"] for t in twitter_samples.strings(\"positive_tweets.json\")] + \n",
    "             [[t, \"negative\"] for t in twitter_samples.strings(\"negative_tweets.json\")])\n",
    "\n",
    "Tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#',\n",
       "  'FollowFriday',\n",
       "  '@',\n",
       "  'France_Inte',\n",
       "  '@',\n",
       "  'PKuchly57',\n",
       "  '@',\n",
       "  'Milipol_Paris',\n",
       "  'for',\n",
       "  'being',\n",
       "  'top',\n",
       "  'engaged',\n",
       "  'members',\n",
       "  'in',\n",
       "  'my',\n",
       "  'community',\n",
       "  'this',\n",
       "  'week',\n",
       "  ':',\n",
       "  ')'],\n",
       " ['@',\n",
       "  'Lamb2ja',\n",
       "  'Hey',\n",
       "  'James',\n",
       "  '!',\n",
       "  'How',\n",
       "  'odd',\n",
       "  ':',\n",
       "  '/',\n",
       "  'Please',\n",
       "  'call',\n",
       "  'our',\n",
       "  'Contact',\n",
       "  'Centre',\n",
       "  'on',\n",
       "  '02392441234',\n",
       "  'and',\n",
       "  'we',\n",
       "  'will',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'assist',\n",
       "  'you',\n",
       "  ':',\n",
       "  ')',\n",
       "  'Many',\n",
       "  'thanks',\n",
       "  '!']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing the words for further processing.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Tweets_token = Tweets.copy()\n",
    "\n",
    "for i in range(len(Tweets_token)):\n",
    "    Tweets_token[i] = word_tokenize(Tweets[i][0])\n",
    "\n",
    "Tweets_token[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#',\n",
       "  'FollowFriday',\n",
       "  '@',\n",
       "  'France_Inte',\n",
       "  '@',\n",
       "  'PKuchly57',\n",
       "  '@',\n",
       "  'Milipol_Paris',\n",
       "  'top',\n",
       "  'engaged',\n",
       "  'members',\n",
       "  'community',\n",
       "  'week',\n",
       "  ':',\n",
       "  ')'],\n",
       " ['@',\n",
       "  'Lamb2ja',\n",
       "  'Hey',\n",
       "  'James',\n",
       "  '!',\n",
       "  'How',\n",
       "  'odd',\n",
       "  ':',\n",
       "  '/',\n",
       "  'Please',\n",
       "  'call',\n",
       "  'Contact',\n",
       "  'Centre',\n",
       "  '02392441234',\n",
       "  'able',\n",
       "  'assist',\n",
       "  ':',\n",
       "  ')',\n",
       "  'Many',\n",
       "  'thanks',\n",
       "  '!'],\n",
       " ['@',\n",
       "  'DespiteOfficial',\n",
       "  'listen',\n",
       "  'last',\n",
       "  'night',\n",
       "  ':',\n",
       "  ')',\n",
       "  'As',\n",
       "  'You',\n",
       "  'Bleed',\n",
       "  'amazing',\n",
       "  'track',\n",
       "  '.',\n",
       "  'When',\n",
       "  'Scotland',\n",
       "  '?',\n",
       "  '!']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words (words that do not contribute meaning) from the Tweets.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "Tweets_clean = Tweets.copy()\n",
    "\n",
    "for i in range(len(Tweets_clean)):\n",
    "    Tweets_clean[i]= [word for word in Tweets_token[i] if word not in stop_words]\n",
    "    \n",
    "Tweets_clean[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#',\n",
       "  'followfriday',\n",
       "  '@',\n",
       "  'france_int',\n",
       "  '@',\n",
       "  'pkuchly57',\n",
       "  '@',\n",
       "  'milipol_pari',\n",
       "  'top',\n",
       "  'engag',\n",
       "  'member',\n",
       "  'commun',\n",
       "  'week',\n",
       "  ':',\n",
       "  ')'],\n",
       " ['@',\n",
       "  'lamb2ja',\n",
       "  'hey',\n",
       "  'jame',\n",
       "  '!',\n",
       "  'how',\n",
       "  'odd',\n",
       "  ':',\n",
       "  '/',\n",
       "  'pleas',\n",
       "  'call',\n",
       "  'contact',\n",
       "  'centr',\n",
       "  '02392441234',\n",
       "  'abl',\n",
       "  'assist',\n",
       "  ':',\n",
       "  ')',\n",
       "  'mani',\n",
       "  'thank',\n",
       "  '!'],\n",
       " ['@',\n",
       "  'despiteoffici',\n",
       "  'listen',\n",
       "  'last',\n",
       "  'night',\n",
       "  ':',\n",
       "  ')',\n",
       "  'As',\n",
       "  'you',\n",
       "  'bleed',\n",
       "  'amaz',\n",
       "  'track',\n",
       "  '.',\n",
       "  'when',\n",
       "  'scotland',\n",
       "  '?',\n",
       "  '!']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "Tweets_stem = Tweets.copy()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for i in range(len(Tweets)):\n",
    "    Tweets_stem[i]=[porter.stem(word) for word in Tweets_clean[i]]\n",
    "    \n",
    "Tweets_stem[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Tweets into bags of words.\n",
    "\n",
    "word_list = []\n",
    "\n",
    "for i1 in range(len(Tweets_stem)):\n",
    "    for i2 in range(len(Tweets_stem[i1])):\n",
    "        if Tweets_stem[i1][i2] not in word_list:\n",
    "            word_list.append([Tweets_stem[i1][i2], Tweets_stem[i1].count(Tweets_stem[i1][i2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#', 1], ['followfriday', 1], ['@', 3], ['france_int', 1], ['@', 3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-of-Speech tagging for the Tweets.\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "Tweets_pos_tagged = Tweets.copy()\n",
    "for i in range(len(Tweets)):\n",
    "    Tweets_pos_tagged[i] = pos_tag(Tweets_token[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('#', '#'),\n",
       "  ('FollowFriday', 'NNP'),\n",
       "  ('@', 'NNP'),\n",
       "  ('France_Inte', 'NNP'),\n",
       "  ('@', 'NNP'),\n",
       "  ('PKuchly57', 'NNP'),\n",
       "  ('@', 'NNP'),\n",
       "  ('Milipol_Paris', 'NNP'),\n",
       "  ('for', 'IN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('top', 'JJ'),\n",
       "  ('engaged', 'VBN'),\n",
       "  ('members', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('community', 'NN'),\n",
       "  ('this', 'DT'),\n",
       "  ('week', 'NN'),\n",
       "  (':', ':'),\n",
       "  (')', ')')],\n",
       " [('@', 'JJ'),\n",
       "  ('Lamb2ja', 'NNP'),\n",
       "  ('Hey', 'NNP'),\n",
       "  ('James', 'NNP'),\n",
       "  ('!', '.'),\n",
       "  ('How', 'WRB'),\n",
       "  ('odd', 'JJ'),\n",
       "  (':', ':'),\n",
       "  ('/', 'JJ'),\n",
       "  ('Please', 'NNP'),\n",
       "  ('call', 'VB'),\n",
       "  ('our', 'PRP$'),\n",
       "  ('Contact', 'NNP'),\n",
       "  ('Centre', 'NNP'),\n",
       "  ('on', 'IN'),\n",
       "  ('02392441234', 'CD'),\n",
       "  ('and', 'CC'),\n",
       "  ('we', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('able', 'JJ'),\n",
       "  ('to', 'TO'),\n",
       "  ('assist', 'VB'),\n",
       "  ('you', 'PRP'),\n",
       "  (':', ':'),\n",
       "  (')', ')'),\n",
       "  ('Many', 'JJ'),\n",
       "  ('thanks', 'NNS'),\n",
       "  ('!', '.')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets_pos_tagged[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've gathered lists of positive and negative words from University of Illinois at Chicago (https://ptrckprry.com/course/ssd/data/negative-words.txt)\n",
    "\n",
    "I will go on to check the Tweets against these lists, and score them.\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+',\n",
       " 'abound',\n",
       " 'abounds',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'accessable',\n",
       " 'accessible',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclamation']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = []\n",
    "with open (\"/Users/muduo/Documents/PosWordsList.txt\") as f:\n",
    "    for line in f:\n",
    "        positive_words.append(line.strip())\n",
    "\n",
    "positive_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words = []\n",
    "with open (\"/Users/muduo/Documents/NegWordsList.txt\") as f:\n",
    "    for line in f:\n",
    "        negative_words.append(line.strip())\n",
    "\n",
    "negative_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abound', 'abund', 'abund'] ['2-face', '2-face', 'abnorm', 'abolish', 'abomin']\n"
     ]
    }
   ],
   "source": [
    "# Taking the stems of all positive and negative words\n",
    "\n",
    "pos_stem = positive_words.copy()\n",
    "neg_stem = negative_words.copy()\n",
    "\n",
    "for i in range(len(pos_stem)):\n",
    "    pos_stem[i]=porter.stem(pos_stem[i])\n",
    "\n",
    "for i in range(len(neg_stem)):\n",
    "    neg_stem[i]=porter.stem(neg_stem[i])\n",
    "\n",
    "print(pos_stem[0:5],\n",
    "neg_stem[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abund', 'access', 'acclaim'] ['2-face', 'abnorm', 'abolish', 'abomin', 'abort']\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicates\n",
    "\n",
    "uniques = []\n",
    "\n",
    "for i in pos_stem:\n",
    "    if i not in uniques:\n",
    "        uniques.append(i)\n",
    "        pos_stem = uniques\n",
    "\n",
    "uniques = []\n",
    "   \n",
    "for i in neg_stem:\n",
    "    if i not in uniques:\n",
    "        uniques.append(i)\n",
    "        neg_stem = uniques\n",
    "        \n",
    "print(pos_stem[0:5],\n",
    "neg_stem[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_df = pd.DataFrame(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0         1  Score\n",
      "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...  positive      2\n",
      "1  @Lamb2ja Hey James! How odd :/ Please call our...  positive      1\n",
      "2  @DespiteOfficial we had a listen last night :)...  positive      0\n",
      "3                               @97sides CONGRATS :)  positive      0\n",
      "4  yeaaaah yippppy!!!  my accnt verified rqst has...  positive      2\n",
      "                                                      0         1  Score\n",
      "9994                ill be on soon, I PROMISE :(\\nwaaah  negative      0\n",
      "9995               I wanna change my avi but uSanele :(  negative      0\n",
      "9996                         MY PUPPY BROKE HER FOOT :(  negative     -1\n",
      "9997           where's all the jaebum baby pictures :((  negative      0\n",
      "9998  But but Mr Ahmad Maslan cooks too :( https://t...  negative      0\n"
     ]
    }
   ],
   "source": [
    "Tweets_df[\"Score\"] = 0\n",
    "\n",
    "# Turning off PANDAS SettingWithCopyWarning.\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "for row in range(len(Tweets)):\n",
    "    for word in Tweets_stem[row]:\n",
    "        if word in pos_stem:\n",
    "            Tweets_df.Score[row] += 1\n",
    "            \n",
    "for row in range(len(Tweets)):\n",
    "     for word in Tweets_stem[row]:\n",
    "        if word in neg_stem:\n",
    "             Tweets_df.Score[row] -= 1\n",
    "            \n",
    "print (Tweets_df[0:5])\n",
    "print (Tweets_df[-6:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy of this method, and its effectiveness in guessing whether the Tweets are positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Showing both histograms.\n",
    "plt.hist(Tweets_df.Score[Tweets_df[1]==\"positive\"], alpha = 0.5)\n",
    "plt.hist(Tweets_df.Score[Tweets_df[1]==\"negative\"], alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive Scores: count    5000.00000\n",
      "mean        0.80660\n",
      "std         1.19293\n",
      "min        -4.00000\n",
      "25%         0.00000\n",
      "50%         1.00000\n",
      "75%         1.00000\n",
      "max         7.00000\n",
      "Name: Score, dtype: float64\n",
      "\n",
      "Negative Scores: count    5000.000000\n",
      "mean        0.028200\n",
      "std         1.141782\n",
      "min        -5.000000\n",
      "25%        -1.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         6.000000\n",
      "Name: Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Showing descriptive statistics for the data\n",
    "\n",
    "print(\"Postive Scores: {}\".format(Tweets_df.Score[Tweets_df[1]==\"positive\"].describe()))\n",
    "print(\"\\nNegative Scores: {}\".format(Tweets_df.Score[Tweets_df[1]==\"negative\"].describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode for both Tweets' scores are 0, not very significant here. There are more negative Tweets with a score of 0 than there are positive Tweets.\n",
    "The mean of scores for positive Tweets are higher than that for negative Tweets by around 0.73. Meaning that positive Tweets contain around 0.73 positive words and/or 0.73 less negative words than negative Tweets.\n",
    "**********************************************************************************************************************\n",
    "Finally, I will test the accuracy of the method through predicting the category of the data by its score.\n",
    "\n",
    "If the score is below 0, the model will predict that the Tweet to be negative. If the score is above 0, the model will predict the Tweet to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2771\n",
      "1448\n"
     ]
    }
   ],
   "source": [
    "correct_pos_predicts = 0\n",
    "\n",
    "correct_neg_predicts = 0\n",
    "\n",
    "for i in range(len(Tweets_df)):\n",
    "    if Tweets_df.Score[i] > 0:\n",
    "        if Tweets_df[1][i] == \"positive\":\n",
    "            correct_pos_predicts += 1\n",
    "    if Tweets_df.Score[i] < 0:\n",
    "        if Tweets_df[1][i] == \"negative\":\n",
    "            correct_neg_predicts += 1 \n",
    "        \n",
    "print (correct_pos_predicts)\n",
    "print (correct_neg_predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 10,000 Tweets, this method predicts the sentiment of Tweet 2771+1448 = 4219 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "     Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\"<br>\n",
    "          Proceedings of the ACM SIGKDD International Conference on Knowledge<br>\n",
    "          Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle,<br>\n",
    "          Washington, USA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
